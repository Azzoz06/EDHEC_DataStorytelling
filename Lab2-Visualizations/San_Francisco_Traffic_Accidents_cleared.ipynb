{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analyze data and build a dashboard with Spark, notebooks, and PixieDust</h1> <br/>\n",
    "\n",
    "Interactive notebooks are powerful tools for fast and flexible experimentation and data analysis. Notebooks can contain live code, static text, equations and visualizations. \n",
    "\n",
    "In this lab, we will walk through how to use PixieDust with Spark and notebooks to:\n",
    "- Analyze open data around traffic accidents in San Francisco\n",
    "- Build charts and maps to discover insights\n",
    "\n",
    "We will then show how to:\n",
    "- Build a dashboard that drills down into specific areas\n",
    "- Combine multiple data sources like crime or speeding zones to extract even more insights  \n",
    "\n",
    "![pixiedust](https://developer.ibm.com/clouddataservices/wp-content/uploads/sites/85/2017/03/pixiedust200.png)\n",
    "\n",
    "Learn more about PixieDust [Here](https://www.ibm.com/analytics/us/en/watson-data-platform/pixiedust/).\n",
    "\n",
    "You can access the complete tutorial with step by step instructions here: <a href=\"https://www.slideshare.net/DTAIEB/pixie-dust-overview\" target=\"_blank\" rel=\"noopener no referrer\">PixieDust overview</a>.\n",
    "\n",
    "This notebook runs on Python with Spark 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Install PixieDust](#install)\n",
    "2. [Import San Francisco traffic accidents data into the notebook](#import)\n",
    "3. [Explore the data for immediate insights](#explore)\n",
    "4. [More data exploration and hypothesis](#morexplore)\n",
    "5. [Focus on the Taraval police district using some friendly SQL notation](#taravalpolice)\n",
    "6. [Further questions](#furtherquestions)\n",
    "7. [Build the PixieApp Dashboard](#dashboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Procure PixieDust <a class=\"anchor\" id=\"install\"></a>\n",
    "Note that DSX Jupyter environment comes with a version of pixiedust already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On a non-DSX notebook, we would install pixiedust with the commands on the following line\n",
    "#!pip install --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order ot use the PixieDust library, it must be imported into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook requires version 1.0.6 or higher\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import San Francisco traffic accidents data into the notebook<a class=\"anchor\" id=\"import\"></a>\n",
    "Source: <a href=\"https://datasf.org/opendata\" target=\"_blank\" rel=\"noopener no referrer\">San Francisco Open Data</a>\n",
    "> Take a moment to explore all the data available at this site\n",
    "\n",
    "With PixieDust, you can easily load CSV data from a URL into a PySpark DataFrame in the notebook.   \n",
    "You will notice that PixieDust will run Spark jobs to get the data into the Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# With PixieDust, you can easily load CSV data from a URL into a PySpark DataFrame in the notebook.\n",
    "\n",
    "accidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/vv57-2fgy/rows.csv?accessType=DOWNLOAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactively Explore the data for immediate insights<a class=\"anchor\" id=\"explore\"></a>\n",
    "\n",
    "After successfully importing PixieDust and loading the sample data, we can use the `display()` API to quickly browse through and visualize the data to see if we can obtain any immediate insights.\n",
    "\n",
    "1. Explore the schema and browse the data\n",
    "   * Select _DataFrame Table_ icon (leftmost htat looks like a grid or table) in the display widget\n",
    "   * This yields a tabular view of the data. This view can be scrolled through\n",
    "\n",
    "2. Explore the data graphically to answer questions, e.g. In which police district do the most traffic accidents occur?\n",
    "   * Choose the _Chart_ icon in the display widget below, select `Pie Chart` type\n",
    "   * Open the Options and verify that the settings are `Keys=PdDistrict`, `Values=IncidntNum`, `Aggregation=Count`\n",
    "   * Th eresulting pie chart shows that `Taraval`, `Southern` and `Mission` are the districts where there are most incidents.\n",
    "\n",
    "3. We can now dig one level deeper by clustering by how each accident was resolved:\n",
    "   * Choose again _Chart_ icon in the display widget and select `Bar Chart` type\n",
    "      * Note that you may get an error stating that `bokeh` library is back level, in this case switch the Renderer back to matplotlib (top right drop-down list)\n",
    "   * Open the options and check that `Keys = PdDistrict`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * On the right side, make sure that the setting is **`Cluster By: Resolution`**\n",
    "   * We see there that `Taraval` and `Mission` still have the most unresolved incidents, while `Mission` has a relatively better elucidation ratio\n",
    "\n",
    "4. we can also investigate on what day of the week do the most traffic accidents occur?\n",
    "   * Choose the _Chart_ icon in the display widget and select `Bar Chart`\n",
    "   * Change the Options (by drag&drop) so that `Keys = DayOfWeek`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Set `Cluster By` back to None to get overall figures, wednesday has slightly higher rates\n",
    "   * Set `Cluster By` to  `PdDistrict` to show by-district figures, where we see that `Taraval` and `Southern` have a spike on Thursdays, while the Wednesday higher average is contributed to mostly by `Bayview` district.\n",
    "\n",
    "As you can see, PixieDust allows to quickly get insights by interactive graphical data exploration, right out of the Data Science notebook environment, and without extensive beforehand processing of the data.\n",
    "\n",
    "You may want to explore the possibility of the Display API by watching this <a href=\"https://www.youtube.com/watch?v=FoOHFlkCaXI\" target=\"_blank\" rel=\"noopener no referrer\">Video</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "handlerId": "barChart",
      "keyFields": "DayOfWeek",
      "mpld3": "false",
      "rendererId": "matplotlib",
      "rowCount": "1000",
      "table_noschema": "false",
      "valueFields": "IncidntNum"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Invoke PixieDust to propose an initial display of the `accidents` Spark DataFrame\n",
    "display(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. More data exploration and hypothesis<a class=\"anchor\" id=\"moreexplore\"></a>\n",
    "We are able to immediately identify a couple of areas of interest in our data without writing a single line of code:\n",
    "- Most accidents happen in the Southern and Taraval police districts\n",
    "- Most accidents happen on Wednesdays and Thursdays\n",
    "\n",
    "Now, the advantage of running this visual exploration from withing the Data Science environment is that it allows to quickly decide and apply further data processing and refinement to drill-down into more analysis.\n",
    "\n",
    "Do go a step further, We can also see that our data needs some cleansing if we want to make analysis easier. Specifically:\n",
    "- The `Time` field is a string, so we'll need to add an `Hour` column if we want to see the time of day when most accidents occur\n",
    "- The `DayOfWeek` values are rendered in alphabetical order by default instead of chronological order, so we should rename them to make it easier to see how the number of accidents changes over the course of the week\n",
    "- We should condense the outcome types of each traffic accident if we want to see the most common resolutions of traffic accidents in each police district, because the clustering above was unclear\n",
    "\n",
    "Let's cleanse the data and re-investigate before moving on:\n",
    "\n",
    "**Note**: the next cell is using PySpark APIs to manipulate the data. You can find more information on these APIs <a href=\"http://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Get the hour value of a time string\n",
    "# e.g. getHour(\"05:30\") = 5\n",
    "def getHour(s):\n",
    "    return int(s.split(':')[0])\n",
    "\n",
    "hr_udf = udf(getHour,IntegerType())\n",
    "\n",
    "# Rename weekdays to enable mini time-series analysis\n",
    "accidents = accidents.na.replace\\\n",
    "    (['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],\\\n",
    "    ['1-Mon','2-Tues','3-Wed','4-Thur','5-Fri','6-Sat','7-Sun'],\\\n",
    "    'DayOfWeek')\n",
    "\n",
    "# Add Hour column and refine outcomes from traffic accidents\n",
    "accidents = accidents.withColumn(\"Hour\",hr_udf(accidents['Time']))\\\n",
    "    .withColumn(\"Res\",\\\n",
    "    udf(lambda x: 'Arrest' if 'ARREST' in x else 'No Resolution' if x == 'NONE' else 'Other',StringType())\\\n",
    "    (accidents['Resolution']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### We are now ready for more data exploration\n",
    "1. Hypothesis: Do accidents in one police district result in more arrests than other police districts?\n",
    "    * To find out, run the cell below and set the following display options:\n",
    "    * Bar Chart\n",
    "    * _Options_: `Keys = PdDistrict`, `Values = IncidntNum`, `Aggregation = Count`, `Cluster By: Res`\n",
    "    * The districts where there are more arrests than no resolution stand out (`Richmnond`, `Mission`, ...)\n",
    "\n",
    "2. Question: How does the number of accidents change over the course of the week?\n",
    "    * To answer, change the options to \n",
    "    * Line Chart\n",
    "    * _Options: `Keys = DayOfWeek`, `Values = IncidntNum`, `Aggregation = Count`,  `Cluster By: None`\n",
    "    * as in the previous graph, Wednesday stands out, but we see a decrease until Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "chartsize": "90",
      "charttype": "grouped",
      "handlerId": "lineChart",
      "keyFields": "DayOfWeek",
      "rendererId": "matplotlib",
      "rowCount": "500",
      "valueFields": "IncidntNum"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we learned\n",
    "A few lines of code makes it a lot easier to see that:\n",
    "\n",
    "- Accidents in the Mission police district are much more likely to result in arrest than all other districts\n",
    "\n",
    "- The number of accidents peaks during the middle of the week, but decreases afterwards as the week winds down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Focus on the Taraval police district using some friendly SQL notation<a class=\"anchor\" id=\"taravalpolice\"></a>\n",
    "We will now use the power of Spark SQL to run familiar syntax SQL queries on the Spark DataFrames   \n",
    "First we need to register the dataset under a name to use as SQL tablename   \n",
    "Since we want to refine exploration to `Taraval`, we will extract the subset and display it using PixieDust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.registerTempTable(\"accidents\")\n",
    "taraval = sqlContext.sql(\"SELECT * FROM accidents WHERE PdDistrict='TARAVAL'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to display a Taraval-focused map\n",
    "We are now able to drill down deeper in the structure of `Taraval` incidents\n",
    "1. Question: Where in Taraval do most accidents happen?\n",
    "   * Select `Map` as type (we have x,y coordinates for incidents locations)\n",
    "   * Set Options to  `Keys = [X,Y]`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Set the Renderer to `mapbox`, kind: chloropleth-cluster\n",
    "   * It appears that incidents are pretty much evenly distributed within the district, outside of lake and hills areas with a slight predominance of south-west, and mostly on main thoroughfares\n",
    "\n",
    "2. Question: What time of day do most accidents occur?\n",
    "   * select type back to `Line Chart`\n",
    "   * Set Options: `Keys = Hour`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * The spike of incidents clearly matches the 2pm-3pm time slot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "basemap": "light-v9",
      "colorrampname": "Orange to Purple",
      "handlerId": "lineChart",
      "keyFields": "Hour",
      "kind": "choropleth-cluster",
      "mapboxtoken": "pk.eyJ1IjoicmFqcnNpbmdoIiwiYSI6ImNqM2s4ZDg4djAwcGYyd3BwaGxwaDV3bWoifQ.d5Rklkdu5MeGAnXu1GMNYw",
      "rendererId": "mapbox",
      "rowCount": "600",
      "timeseries": "false",
      "valueFields": "IncidntNum"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(taraval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we learned\n",
    "Most of the results from looking at the accident times are unsurprising:\n",
    "\n",
    "- Fewer accidents during very early morning (people probably sleeping)\n",
    "- Steady increase in number of accidents during morning commuting hours\n",
    "- Fewer accidents during mid-evening (people probably eating dinner)\n",
    "- (Sadly) more accidents late at night\n",
    "\n",
    "The interesting thing here is the sudden spike in accidents during mid-afternoon (2-3PM) - twice as many accidents happen during this two-hour window!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Further questions<a class=\"anchor\" id=\"furtherquestions\"></a>\n",
    "\n",
    "In analyzing the geographical data, we can see a couple of clusters where accidents occur more frequently in Taraval - the southeastern corner looks particularly crowded. Some useful questions to ask at this point are:\n",
    "\n",
    "**- Does crime have an effect on the number of accidents?**\n",
    "\n",
    "**- Are there more accidents in these areas because more people speed there?**\n",
    "\n",
    "**- Do traffic calming devices reduce the number of accidents?**\n",
    "\n",
    "A Data Scientist would test these hypotheses by downloading datasets for speeding data and traffic calming in San Francisco, join this to the existing data through Spark DataFrame work, and use the `display` API to visualize speeding zones and areas with traffic calming devices separately"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0 (Deprecated)",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
