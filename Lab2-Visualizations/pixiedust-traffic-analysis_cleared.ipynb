{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Watson Studio with PixieDust\n",
    "\n",
    "### Analyze data and build a dashboard with Spark, notebooks, and PixieDust\n",
    "\n",
    "Interactive notebooks are powerful tools for fast and flexible experimentation and data analysis. Notebooks can contain live code, static text, equations and visualizations. \n",
    "\n",
    "In this lab, we will walk through how to use PixieDust with Spark and notebooks to:\n",
    "- Analyze open data around traffic accidents in San Francisco\n",
    "- Build charts and maps to discover insights\n",
    "\n",
    "We will then show how to:\n",
    "- Build a dashboard that drills down into specific areas\n",
    "- Combine multiple data sources like crime or speeding zones to extract even more insights  \n",
    "\n",
    "![pixiedust](https://developer.ibm.com/clouddataservices/wp-content/uploads/sites/85/2017/03/pixiedust200.png)\n",
    "\n",
    "Learn more about PixieDust [Here](https://www.ibm.com/analytics/us/en/watson-data-platform/pixiedust/).\n",
    "\n",
    "This notebook runs on Python with Spark 2.x.\n",
    "\n",
    "This Lab is based on the tutorial published with step by step instructions here: [https://www.slideshare.net/DTAIEB/pixie-dust-overview](https://www.slideshare.net/DTAIEB/pixie-dust-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade bokeh==0.12.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Procure PixieDust <a class=\"anchor\" id=\"install\"></a>\n",
    "Note that **IBM Watson Studio** Jupyter environment comes with a version of pixiedust already installed.  \n",
    "In the next cell, we will import the `pixiedust` package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before, you can use the PixieDust library it must be imported into the notebook.\n",
    "# This notebook requires version 1.0.6\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import San Francisco Police incidents data into the Notebook\n",
    "Source: [San Francisco Open Data](https://datasf.org/opendata)\n",
    "> You may want to take a moment to explore all the data available at this site\n",
    "\n",
    "We will use pixiedust to easily load CSV data from a URL into a PySpark DataFrame.\n",
    "\n",
    "You will notice the `SPARK JOB PROGRESS` window that shows the status of the various Spark jobs launched by pixiedust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a PySpark dataframe from its URL\n",
    "#dfIncidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/956q-2t7k/rows.csv?accessType=DOWNLOAD\")\n",
    "#dfIncidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/wg3w-h783/rows.csv?accessType=DOWNLOAD\")\n",
    "dfIncidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration\n",
    "\n",
    "After successfully importing PixieDust and loading the sample data, we can use the `display()` API to quickly browse through and visualize the data to obtain immediate insights.\n",
    "\n",
    "Run the cell with below, then apply the following steps in the interactive output which will be created below:\n",
    "\n",
    "1. Explore the schema and browse the data\n",
    "   * Select _DataFrame Table_ icon (leftmost that looks like a grid or table) in the display widget\n",
    "   * This yields a tabular view of the data. This view can be scrolled through\n",
    "\n",
    "2. Explore the data graphically to answer questions, e.g. In which police district do the most police incidents occur?\n",
    "   * Choose the _Chart_ icon in the display widget below, select `Pie Chart` type\n",
    "   * Open the Options and verify that the settings are `Keys=PdDistrict`, `Values=IncidntNum`, `Aggregation=Count`\n",
    "   * The resulting pie chart shows that `Southern`, `Mission` and `Northern` are the districts where there are most incidents.\n",
    "\n",
    "3. We can now dig one level deeper by clustering by how each accident was resolved:\n",
    "   * Choose again _Chart_ icon in the display widget and select `Bar Chart` type\n",
    "      * Note that you may get an error stating that `bokeh` library is back level, in this case switch the Renderer back to matplotlib (top right drop-down list)\n",
    "   * Open the options and check that `Keys = PdDistrict`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * On the right side, make sure that the setting is **`Cluster By: Resolution`**\n",
    "   * You may want to uncheck the ' show legend' box to fully view the bars.\n",
    "   * We notice there that `Southern`, `Northern`, `Mission` and `Central` have the most unresolved incidents, while `Southern` and `Mission` has a relatively higher arrest count.\n",
    "   * You can switch to `Type` : `Stacked` to compare cumulated incidents. \n",
    "\n",
    "4. we can also investigate on what day of the week do the most police incidents occur:\n",
    "   * Choose the _Chart_ icon in the display widget and select `Bar Chart`\n",
    "   * Change the Options (by drag&drop) so that `Keys = DayOfWeek`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Set `Cluster By` back to None to get overall figures, which shows that all days are very similar\n",
    "   * Since the height of the bars are so similar, use `Pie Chart - Options: Keys = DayOfWeek, Values = IncidntNum, Aggregation = Count)`, which confirms that each day has almost its equal share of 13-15% of the total.\n",
    "\n",
    "You may want to take a moment to explore the possibility of the Display API by watching this [video](https://www.youtube.com/watch?v=FoOHFlkCaXI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "chartsize": "99",
      "charttype": "grouped",
      "clusterby": "Resolution",
      "handlerId": "tableView",
      "keyFields": "DayOfWeek",
      "legend": "false",
      "mpld3": "false",
      "rendererId": "bokeh",
      "rowCount": "1000",
      "sortby": "Values ASC",
      "valueFields": "IncidntNum"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(dfIncidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data Exploration and Hypothesis\n",
    "As we have just seen, we can quickly identify a couple of areas of interest in our data without having to write a single line of code:\n",
    "\n",
    "1) Most incidents happen in the Southern  police district, and\n",
    "\n",
    "2) The number of incidents is approximately the same for each day, ranging from 13-15% of the total per day.\n",
    "\n",
    "When looking at the table voew, we also realized that our data needs some cleansing if we want to make analysis easier. Specifically:\n",
    "\n",
    "- The `Time` field is a string, so we'll need to add an `Hour` column if we want to see the time of day when most incidents occur, and\n",
    "- The `DayOfWeek` values are rendered in alphabetical order by default instead of chronological order, so we should rename them to make it easier to see how the number of incidents changes over the course of the week\n",
    "- And we should condense the outcome types of each police incident if we want to see the most common resolutions of police incidents in each police district, since the clustering above was undifferentiating. We will create a calculated column `Res` \n",
    "\n",
    "Let's cleanse the data using Spark DataFrame code and re-investigate before moving on:\n",
    "\n",
    "> Note: the next cell is using PySpark APIs to manipulate the data. You can find more information on these APIs [here](http://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Rename weekdays to sorted names in order to enable mini time-series analysis\n",
    "dfIncidents = dfIncidents.na.replace\\\n",
    "    (['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],\\\n",
    "    ['1-Mon','2-Tues','3-Wed','4-Thur','5-Fri','6-Sat','7-Sun'],\\\n",
    "    'DayOfWeek')\n",
    "\n",
    "# Get the hour value of a time string, e.g. getHour(\"05:30\") will yield the integer value 5\n",
    "def getHour(s):\n",
    "    return int(s.split(':')[0])\n",
    "# We use this python function to define a user-defined-function that can be executed in the Spark cluster\n",
    "hr_udf = udf(getHour,IntegerType())\n",
    "\n",
    "# Add Hour column and refine outcomes from police incidents using the UDF\n",
    "dfIncidents = dfIncidents.withColumn(\"Hour\",hr_udf(dfIncidents['Time']));\n",
    "\n",
    "# Similarly, use a udf  to coalesce incident resolution as `Arrest`, `No Resolution`  or `Other`\n",
    "dfIncidents = dfIncidents.withColumn(\"Res\",udf(lambda x: 'Arrest' if 'ARREST' in x else 'No Resolution' if x == 'NONE' else 'Other',StringType())(dfIncidents['Resolution']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are now ready for more data exploration\n",
    "\n",
    "Run the cell below which operates on the augmented dataframe\n",
    "\n",
    "1. Hypothesis: Do incidents in one police district result in more arrests than other police districts?\n",
    "    * To find out, run the cell below and set the following display options:\n",
    "    * Bar Chart\n",
    "    * _Options_: `Keys = PdDistrict`, `Values = IncidntNum`, `Aggregation = Count`, `Cluster By: Res`\n",
    "    * The districts where there are more arrests than no resolution stand out (`Richmnond`, `Mission`, ...)\n",
    "    I find the `horizontal` orientation better for showing the Police Districts.\n",
    "\n",
    "2. Question: How does the number of incidents change over the course of the week?\n",
    "    * To answer, change the options to \n",
    "    * Line Chart\n",
    "    * _Options: `Keys = DayOfWeek`, `Values = IncidntNum`, `Aggregation = Count`,  `Cluster By: None`\n",
    "    * We now see a slow increase to a slight peak on Fridays, with a decrease until Sundays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "chartsize": "90",
      "charttype": "grouped",
      "handlerId": "lineChart",
      "keyFields": "DayOfWeek",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "500",
      "valueFields": "IncidntNum"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(dfIncidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIncidents.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learned\n",
    "A few lines of code makes it a lot easier to see that:\n",
    "\n",
    "1) Incidents in the Mission and Southern police districts are much more likely to result in arrest than all other districts, and\n",
    "\n",
    "2) The number of incidents raises slightly during the middle of the week, through Friday, and then decreases through Sunday.\n",
    "\n",
    "## Now let's focus on the Mission police district using some friendly SQL notation\n",
    "PySpark and SparkSQL allow for high-level SQL-like queries to run on Spark DataFrames.   \n",
    "We use this here to create a new `dfMission` dataframe which has only Mission PdDistrict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIncidents.registerTempTable(\"incidents\")\n",
    "dfMission = sqlContext.sql(\"SELECT * FROM incidents WHERE PdDistrict='MISSION'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to display a Mission-focused map\n",
    "We are now able to drill down deeper in the structure of `Mission` incidents\n",
    "1. Question: Where in Mission do most incidents happen?\n",
    "   * Select `Map` as type (we have x,y coordinates for incidents locations)\n",
    "   * Set Options to  `Keys = [X,Y]`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Set the Renderer to `mapbox`, kind: `density-map`\n",
    "   * It appears that incidents are distributed with a predominance along the district's central street 'Mission St', with a predominance on main streets and avenues.\n",
    "\n",
    "2. Question: What time of day do most incidents occur?\n",
    "   * select type back to `Line Chart`\n",
    "   * Set Options: `Keys = Hour`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Two spikes of incidents clearly matches noon and evening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "basemap": "light-v9",
      "chartsize": "91",
      "handlerId": "lineChart",
      "keyFields": "Hour",
      "kind": "choropleth-cluster",
      "mapboxtoken": "pk.eyJ1IjoicmFqcnNpbmdoIiwiYSI6ImNqM2s4ZDg4djAwcGYyd3BwaGxwaDV3bWoifQ.d5Rklkdu5MeGAnXu1GMNYw",
      "rendererId": "mapbox",
      "rowCount": "1000",
      "timeseries": "false",
      "valueFields": "IncidntNum"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(dfMission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learned:\n",
    "Most of the results from looking at the incident times are unsurprising:\n",
    "\n",
    "- Number of incidents drop sharply very early morning (people probably sleeping),\n",
    "- Steady increase in number of incidents until noon,\n",
    "- Fairly high numbers from 3:00 PM until 8:00 PM,\n",
    "- Surprisingly, incidents decline after 8:00PM.\n",
    "\n",
    "The interesting thing here is the fact that the peaks are at noon and from 3:00PM until 8:00 PM, as one might expect the later evening times to be more problematic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the pixiedust lab. In the nextlab, we will se Watson Studio can be used to build dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
